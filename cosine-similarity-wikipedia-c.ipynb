{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity (Version C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy:   1.19.2\n",
      "sklearn: 0.23.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "\n",
    "# https://hobbitdata.informatik.uni-leipzig.de/EML4U/2021-02-10-Wikipedia-Texts/\n",
    "source_texts_directory = \"/home/eml4u/EML4U/data/corpus/2021-02-10-wikipedia-texts/\"\n",
    "# https://hobbitdata.informatik.uni-leipzig.de/EML4U/2021-04-07-Wikipedia-Embeddings/\n",
    "embeddings_directory  = \"/home/eml4u/EML4U/data/wikipedia-embeddings/\"\n",
    "\n",
    "# points of time\n",
    "id_a = \"20100408\"\n",
    "id_b = \"20201101\"\n",
    "# category ids\n",
    "id_american = \"american-films\"\n",
    "id_british  = \"british-films\"\n",
    "id_indian   = \"indian-films\"\n",
    "# file ids\n",
    "id_american_a = id_a + \"-\" + id_american\n",
    "id_american_b = id_b + \"-\" + id_american\n",
    "id_british_a  = id_a + \"-\" + id_british\n",
    "id_british_b  = id_b + \"-\" + id_british\n",
    "id_indian_a   = id_a + \"-\" + id_indian\n",
    "id_indian_b   = id_b + \"-\" + id_indian\n",
    "\n",
    "\n",
    "# Imports\n",
    "\n",
    "import numpy\n",
    "print(\"numpy:   \" + numpy.version.version)\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "print(\"sklearn: \" + sklearn.__version__)\n",
    "\n",
    "# Class instance to access data (wp texts, pre-computed embeddings)\n",
    "import data_access\n",
    "data_accessor = data_access.DataAccess(source_texts_directory, embeddings_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Average embeddings\n",
    "\n",
    "Compute average embeddings for 2 points in time. The results will be a 768-dimensional vector for each point in time.  \n",
    "→ Get texts compared to the average vectors.\n",
    "\n",
    "→ Get typical texts    \n",
    "* One vector of old point in time $\\bar{v_{t1}}$, one vector new point in time $\\bar{v_{t2}}$  \n",
    "* Between: CosSim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/eml4u/EML4U/data/wikipedia-embeddings/20100408-british-films.txt\n",
      "(2147, 768) <class 'numpy.ndarray'>\n",
      "/home/eml4u/EML4U/data/wikipedia-embeddings/20201101-british-films.txt\n",
      "(2147, 768) <class 'numpy.ndarray'>\n",
      "\n",
      "Average embeddings for 2 points in time:\n",
      "<class 'numpy.ndarray'> (768,) BritishA\n",
      "<class 'numpy.ndarray'> (768,) BritishB\n"
     ]
    }
   ],
   "source": [
    "# Load embeddings\n",
    "\n",
    "embeddings_british_a = data_accessor.load_embeddings(id_british_a)\n",
    "embeddings_british_b = data_accessor.load_embeddings(id_british_b)\n",
    "print()\n",
    "\n",
    "\n",
    "# Compute means / average embeddings\n",
    "\n",
    "def get_mean(embeddings, note = \"\", printinfo = True):\n",
    "    mean = numpy.mean(embeddings, axis=0)\n",
    "    if printinfo:\n",
    "        print(str(type(mean)) + \" \" + str(mean.shape) + \" \" +  note)\n",
    "    return mean\n",
    "\n",
    "print(\"Average embeddings for 2 points in time:\")\n",
    "mean_british_a  = get_mean(embeddings_british_a, \"BritishA\")\n",
    "mean_british_b  = get_mean(embeddings_british_b, \"BritishB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity of average embeddings for 2 points in time:\n",
      "<class 'numpy.ndarray'> (1, 768)\n",
      "<class 'numpy.ndarray'> (1, 768)\n",
      "0.9936639191853995 British\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarity\n",
    "\n",
    "def get_pairwise_cosine_similarity(a, b, note = \"\", printinfo = True):\n",
    "    if printinfo:\n",
    "        print(str(type(a)) + \" \" + str(a.shape) + \"\\n\" + str(type(b)) + \" \" + str(b.shape))\n",
    "    cosSim = sklearn.metrics.pairwise.cosine_similarity(a, b, dense_output=True)[0][0]\n",
    "    if printinfo:\n",
    "        print(str(cosSim) + \" \" + note)\n",
    "    return cosSim\n",
    "\n",
    "print(\"Cosine similarity of average embeddings for 2 points in time:\")\n",
    "british_mean_cosine = get_pairwise_cosine_similarity(mean_british_a.reshape(1, -1), mean_british_b.reshape(1, -1), \"British\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Typical texts:\n",
      "768 [(721, 0.8095643688026439), (333, 0.8195543132287988), (680, 0.820176887350935), (663, 0.8234479285109668), (406, 0.834516265316151)]\n",
      "768 [(680, 0.7901596223863818), (333, 0.8048592466102065), (179, 0.806587230956004), (381, 0.8126745126594594), (245, 0.8173266825145751)]\n"
     ]
    }
   ],
   "source": [
    "# Texts compared to the average vectors\n",
    "\n",
    "print(\"Typical texts:\")\n",
    "similarities_british_a = []\n",
    "for i in range(len(mean_british_a)):\n",
    "    similarities_british_a.append((i, get_pairwise_cosine_similarity(  mean_british_a.reshape(1, -1), embeddings_british_a[i].reshape(1, -1), \"\", False  )))\n",
    "similarities_british_a = sorted(similarities_british_a, key=lambda tup: tup[1], reverse=False)\n",
    "print(len(similarities_british_a), similarities_british_a[0:5])\n",
    "\n",
    "similarities_british_b = []\n",
    "for i in range(len(mean_british_b)):\n",
    "    similarities_british_b.append((i, get_pairwise_cosine_similarity(  mean_british_b.reshape(1, -1), embeddings_british_b[i].reshape(1, -1), \"\", False  )))\n",
    "similarities_british_b = sorted(similarities_british_b, key=lambda tup: tup[1], reverse=False)\n",
    "print(len(similarities_british_b), similarities_british_b[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print source texts\n",
    "def print_source_text(directory, category_id, index):\n",
    "    print()\n",
    "    print(\"Category: \" + category_id)\n",
    "    print(\"Index:    \" + str(index))\n",
    "    file = data_accessor.get_embeddings_dict_filename(category_id, index);\n",
    "    print(\"File:     \")\n",
    "    print(data_accessor.read_source_text(directory, file))\n",
    "    print()\n",
    "\n",
    "if False:\n",
    "    print_source_text(id_british_b, id_british, similarities_british_b[0][0])\n",
    "    print_source_text(id_british_b, id_british, 680)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compare each document embedding $v_{t2i}$ (of every wp article) at $t2$ with $\\bar{v_{t2}}$ using CosSim.  \n",
    "\n",
    "* Get WP articles with largest distance to mean-vector $\\bar{v_t2}$.\n",
    "* Optional: For article with largest distance, check attention and highlight words with largest attention  \n",
    "e.g. Integrated Gradients for text https://github.com/SeldonIO/alibi\n",
    "* Check plotting + word counts (end of file) https://github.com/EML4U/Topic-Modeling/blob/main/Twitter%20test.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get articles with largest distance to v_t2\n",
    "# Distance: Smallest cosine similarity\n",
    "# -> See similarities_british_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(680, 0.7901596223863818), (333, 0.8048592466102065), (179, 0.806587230956004), (381, 0.8126745126594594), (245, 0.8173266825145751), (255, 0.8246783158761353), (406, 0.8461331874787608), (663, 0.8509946040235267), (213, 0.8514522727454383), (483, 0.862620813656519), (526, 0.8629482931038279), (101, 0.8641558058376334), (720, 0.8643496013779346), (332, 0.8671766380114813), (605, 0.8679861756571845), (574, 0.8692133375218205), (385, 0.8703120437107627), (400, 0.8710870542779425), (272, 0.8747334865889536), (591, 0.8748916853566279), (295, 0.8764491348254336), (674, 0.880598769307674), (747, 0.8808114412749424), (174, 0.8836400509311044), (259, 0.8854263498204921), (355, 0.8855939571431497), (304, 0.8872719351437507), (764, 0.8881581364107067), (216, 0.8881797777193562), (23, 0.8894833788108552), (290, 0.890238506109883), (183, 0.8910271068032816), (522, 0.8910528699982174), (240, 0.8925691265135722), (727, 0.8929515843282431), (164, 0.8945786418679671), (17, 0.8953558750795475), (545, 0.895993783162472), (607, 0.8965775198164733), (231, 0.8980108917931562), (57, 0.8980762706374805), (184, 0.8982212309904882), (434, 0.8986611119646175), (122, 0.898827195197885), (67, 0.8998663455275321), (556, 0.9003189041970594), (590, 0.9004166407650211), (457, 0.9007004468567859), (346, 0.9010413019557024), (423, 0.9012675153143597), (690, 0.9013733623199528), (5, 0.9020658821344628), (550, 0.9021921230552523), (621, 0.902402755346507), (638, 0.9030787242117865), (750, 0.9034156360975514), (361, 0.903826025688134), (93, 0.9042209045942264), (317, 0.9048591020673289), (181, 0.9050413099825783), (560, 0.9052167271479825), (65, 0.9054069361812698), (422, 0.9059300780395014), (31, 0.9065906663928514), (173, 0.9067909818201796), (212, 0.9068042429409575), (223, 0.9068959772714202), (254, 0.9073279985771328), (722, 0.9085886896821918), (199, 0.90892418798255), (376, 0.9093720072538432), (559, 0.9096005587498609), (323, 0.9096061645018754), (496, 0.9113289948696484), (209, 0.9113291500231548), (49, 0.911336615941549), (102, 0.9115611591288579), (554, 0.9116817155693091), (365, 0.9117075957252481), (226, 0.9118774761440225), (439, 0.9119757206981256), (636, 0.9127183543539972), (530, 0.9130618369750791), (582, 0.91308068507028), (289, 0.9133160007871381), (270, 0.91355497313124), (701, 0.9138913337711323), (735, 0.9146864301731168), (696, 0.9151719336278836), (567, 0.9158435369015614), (415, 0.9160466100811097), (267, 0.9162828990783112), (140, 0.9162910468996968), (597, 0.9164778297480922), (180, 0.916561541634806), (285, 0.917317985229549), (539, 0.9178363852853723), (96, 0.9180244166942362), (357, 0.9180405615351663), (44, 0.918301653957751)]\n"
     ]
    }
   ],
   "source": [
    "# 100 articles with largest distance to mean vector B\n",
    "distant_british_b = similarities_british_b[0:100]\n",
    "print(distant_british_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EML4U)",
   "language": "python",
   "name": "eml4u"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

